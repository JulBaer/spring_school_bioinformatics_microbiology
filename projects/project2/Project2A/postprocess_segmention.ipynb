{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocessing Segmentation\n",
    "Ilastik outputs a probability mask that assigns a probability for each pixel that it either belongs to foreground (cell) or background.\n",
    "We now need to convert this to a proper segmentation, where each cell is assigned a unique number (i.e. a label).\n",
    "\n",
    "> **Exercise**  \n",
    ">  \n",
    "> Think about the steps needed to convert a probability map to a segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages\n",
    "\n",
    "Before starting the code we need to import all the required packages.\n",
    "\n",
    "We use a number of important Python packages:\n",
    "- [Numpy](https://numpy.org): Goto package for vector/matrix based calculations (heavily inspired by Matlab)\n",
    "- [Pandas](https://pandas.pydata.org): Goto package for handling data tables (heavily inspired by R) \n",
    "- [Scipy](https://scipy.org): Numpy extensions for statistics, image analysis, and more\n",
    "- [Scikit-image (skimage)](https://scikit-image.org): Goto package for image analysis\n",
    "- [Matplotlib](https://matplotlib.org): Goto package for plotting data\n",
    "- [Seaborn](https://seaborn.pydata.org): Fancy plots made easy (Similar to ggplot in R)\n",
    "- [Napari](https://napari.org): GUI based interactive image viewer\n",
    "- [Dask-Image](https://image.dask.org/en/latest/): Out-of-memory computation made easy\n",
    "- [pathlib](https://docs.python.org/3/library/pathlib.html): Path handling made easy\n",
    "- [h5py](https://www.h5py.org): Read HDF5 file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "#next two lines make sure that Matplotlib plots are shown properly in Jupyter Notebook\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "#next line is required for Napari\n",
    "%gui qt\n",
    "\n",
    "#main data analysis packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#image processing packages\n",
    "from scipy import ndimage as ndi\n",
    "import skimage.segmentation as segmentation \n",
    "import skimage.filters as filters\n",
    "from skimage.measure import label, regionprops, regionprops_table\n",
    "from skimage import morphology\n",
    "\n",
    "#data plotting packages\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#set default figure size\n",
    "matplotlib.rc(\"figure\", figsize=(10,5))\n",
    "import seaborn as sns\n",
    "\n",
    "#image viewer\n",
    "import napari\n",
    "from napari.utils.notebook_display import nbscreenshot\n",
    "\n",
    "#out of memory computation\n",
    "from dask_image.imread import imread\n",
    "import dask.array as da\n",
    "\n",
    "#path handling\n",
    "import pathlib\n",
    "\n",
    "#file handling\n",
    "import h5py\n",
    "\n",
    "#Instead of dask_image.imread.imread() you can also use tifffile.imread() to directly read images into memory\n",
    "#import tifffile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we initiate a cashe for Dask to speed up repeated computation (important for working with Napari)\n",
    "from dask.cache import Cache\n",
    "cache = Cache(2e9)  # Leverage two gigabytes of memory\n",
    "cache.register()    # Turn cache on globally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and visualize Ilastik output\n",
    "### Load images\n",
    "First we need to specify the folder and file names and some image properties.  \n",
    "If you followed our filename convention you don't have to make any changes here, but if you get an error later on check your path and filenames and adept them in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the path to the folder that contains project data\n",
    "#pathlib.Path.home() return the location of your home folder in a platform independent way. \n",
    "root = pathlib.Path(pathlib.Path.home(), \n",
    "                    'I2ICourse/Project2A/ProcessedData/')\n",
    "\n",
    "image_name = 'pos0_preproc-rg.tif' #set name of image\n",
    "segment_name = 'pos0_preproc-rg_Probabilities.h5' #set name of segmented data\n",
    "n_channel = 2 #set number of color channels in image\n",
    "fg_idx = 0 #set index of foreground label used in Ilastik\n",
    "\n",
    "im_path = root / image_name \n",
    "print('path to image: ', im_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time-lapse images can be big, and can easily overload your memory. To make sure this does not happen, you can use \"out of memory\" computation, where only the data that is directly needed is loaded into memory (e.g. the current time point). This is slower then loading the full dataset into memory, but it scales well when data gets too big to do that. Out-of-memory computation is quite easy with the use of [Dask image](https://image.dask.org/en/latest/). We will use this below.\n",
    "\n",
    "Don't worry about the technical details, though if you are curious we provide some optional 'Technical Notes' below.\n",
    "\n",
    "We now load the image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can use tifffile.imread to read complete image stack into memory.\n",
    "#im_stack = tifffile.imread(im_path) #load image into memory\n",
    "\n",
    "# or dask_image imread to use out of memory processing\n",
    "im_stack = imread(im_path) #load image with dask-image for out of memory processing \n",
    "\n",
    "# dask_image imread creates a 3D stack, where both color channels are interweaved\n",
    "# to separate them we need to reshape to 4D stack\n",
    "if n_channel>1: \n",
    "    newshape = (int(im_stack.shape[0]/n_channel), n_channel, *im_stack.shape[1:])\n",
    "    im_stack = im_stack.reshape(newshape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the data we will use [Napari](https://napari.org), that allows for interactive image visualization. \n",
    "You can use the slider at the bottom of the window to scroll trough time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Technical note: Napari and Dask work very well together, every time you move the time slider, Dask loads the corresponding image into memory.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup napari viewer, with 2 channel image\n",
    "viewer = napari.view_image(im_stack,\n",
    "            channel_axis=1,\n",
    "            name=[\"red\", \"green\"],\n",
    "            colormap=[\"yellow\", \"blue\"],)\n",
    "napari.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and visualize Ilastik output\n",
    "Next we load the data exported by Ilastik.\n",
    "\n",
    "`h5py.File=()` loads a dictionary, which contains a single entry called `exported_data`.   \n",
    "This contains the Ilastik probability output as an array of shape (t,label,y,x).  \n",
    "We only need the probability that a pixel belongs to the foreground label (= cells), so we extract the corresponding layer.\n",
    "\n",
    "Then we add the probability map to the Napari Viewer.\n",
    "You can change the opacity and layer order to improve visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_path = root / segment_name #path to Ilastik output\n",
    "seg_data = h5py.File(seg_path, 'r') #open \n",
    "print('keys = ', list(seg_data.keys()))\n",
    "\n",
    "#we again use Dask to load the data out of memory\n",
    "#technical comment: we set chunk size to be equal to the size of single frame. \n",
    "chuck_size = (1, *im_stack.shape[-2:]) if n_channel==1 else (1, n_channel, *im_stack.shape[-2:])\n",
    "seg_cell = da.from_array(seg_data['exported_data'], chunks=chuck_size)\n",
    "\n",
    "#we get a segmentation probability for both labels (cells and BG) but only need the one for the cells, so we extract the correct dimension:\n",
    "seg_cell = seg_cell[:,fg_idx,:,:]\n",
    "\n",
    "#add probability layer to Napari Viewer\n",
    "prop_layer = viewer.add_image(seg_cell, name='probability',colormap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise** \n",
    ">   \n",
    "> Check the segmentation quality. How well did it work? Are there frames with big problems?\n",
    ">   \n",
    "> If needed, you can go back to Ilastik and add extra training points in problem areas (check with Tutor)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Ilastik probability map into segmentation\n",
    "Ilastik assigns to each pixel the probability that it belongs to a cell. We need to convert this to in instance based segmentation, where each cell is assigned a unique label. This requires a number of steps:\n",
    "1. Pre-process probability map using filters\n",
    "2. Convert probability map to semantic segmentation (binary image where cells=1 and background=0) using thresholding\n",
    "3. Clean up semantic segmentation using morphological operations\n",
    "4. Convert semantic segmentation into instance segmentation (integer images where each cells has own label)\n",
    "5. Post-process instance segmentation by separating merged cells using watershed algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process probability map using filters\n",
    "As a first step probability maps are often processed using a Gaussian blur filter using a small (~1 pixel) size, to ensure that the probability maps are locally smooth. We will use scikit image [`filters.gaussion`](https://scikit-image.org/docs/stable/api/skimage.filters.html) to do this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Technical note: scikit images does not automatically play nice with Dask's out of memory processing, instead we need to manually specify that we want to run `filters.gaussion` on each chunk (i.e. each frame) of our `seg_cell` image stack. This we can do using the Dask Image [`map_blocks()` function](https://docs.dask.org/en/latest/generated/dask.array.map_blocks.html))*.\n",
    "\n",
    "*The usage is pretty straightforward, if normally you would use: `name_of_function_to_apply(normal_array, other_function_arguments)` you can now use: `da.map_blocks(name_of_function_to_apply, dask_array, other_function_arguments)`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 1 #size of Gaussion kernel to use \n",
    "\n",
    "#this is how you normally use scikit filters.gaussian\n",
    "#seg_cell_sm = filters.gaussian(seg_cell, sigma, channel_axis=0) \n",
    "\n",
    "#this is how we do it using Dask:\n",
    "seg_cell_sm = da.map_blocks(filters.gaussian, seg_cell, sigma, channel_axis=0)\n",
    "\n",
    "#add to viewer\n",
    "prop_layer_sm = viewer.add_image(seg_cell_sm, name='probability smoothed',colormap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create semantic segmentation by thresholding probability map\n",
    "\n",
    "[Thresholding](https://en.wikipedia.org/wiki/Thresholding_%28image_processing%29) is used to convert continues probability maps into binary label images. It is a first step in segmenting objects. A threshold value determines the intensity value separating foreground pixels from background pixels. Foregound pixels are pixels brighter than the threshold value, background pixels are darker.\n",
    "\n",
    "There are automatic methods to calculate a threshold, e.g. [Otsu's method](https://en.wikipedia.org/wiki/Otsu%27s_method) and [Li's minimum cross entropy threshold](https://scikit-image.org/docs/dev/auto_examples/developers/plot_threshold_li.html) are two common algorithms. \n",
    "\n",
    "However, here we will pick the threshold manually. \n",
    "\n",
    "Note: to prevent subjectivity to affect your analysis it is important to keep a manually chosen threshold (or the chosen automatic method) constant between replicates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create array with all threshold values to rry, here we use 0,0.01,0.02,...,1\n",
    "thresholds_to_try = np.linspace(0,1,101)\n",
    "\n",
    "#apply treshold, loop over all threshold values and store output in list\n",
    "all_thresholds = [seg_cell_sm>t for t in thresholds_to_try]\n",
    "\n",
    "#convert the list of 3D stacks to a single 4D stack\n",
    "threshold_stack = da.stack(all_thresholds, axis=0)\n",
    "\n",
    "#add to viewer\n",
    "mask_layer_int = viewer.add_image(threshold_stack, name='binary mask int',colormap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Napari viewer you will now have a new layer `binary mask int`, which shows you the result of the interactive thresholding, you can change the threshold value by moving the new slider (channel 0).\n",
    "\n",
    "> **Exercise** \n",
    ">  \n",
    "> Move the slider to find a good threshold value. How does it compare to the one calculated by the algorithms above? \n",
    "> \n",
    "> Hint: the slider position shows the index in the `thresholds_to_try`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Technical aside: if at any time you want to store the current view shown in the Napari viewer, then you can use `nbscreenshot(viewer)`.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbscreenshot(viewer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise** \n",
    "> \n",
    "> Now it is time to choose a final threshold value. Enter the value you have chosen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose your favorite method or enter a manually chosen value\n",
    "final_threshold = 0.66\n",
    "\n",
    "#threshold and add to viewer\n",
    "bin_mask = seg_cell_sm > final_threshold\n",
    "mask_layer_final = viewer.add_image(bin_mask, name='final binary mask',colormap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now remove the interactive threshold layer from the viewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer.layers.remove(\"binary mask int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up semantic segmentation using morphological operations\n",
    "\n",
    "[Mathematical morphology](https://en.wikipedia.org/wiki/Mathematical_morphology) operations and structuring elements are defined in `skimage.morphology`.\n",
    "\n",
    "Functions operating on [connected components](https://en.wikipedia.org/wiki/Connected_space) can remove small undesired elements while preserving larger shapes.\n",
    "\n",
    "`skimage.morphology.remove_small_holes` fills holes and `skimage.morphology.remove_small_objects` removes bright regions. Both functions accept a size parameter, which is the minimum size (in pixels) of accepted holes or objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_hole_size = 40 # maximum area of holes that will be filled (in pixels)\n",
    "min_cell_size = 150 # minimum area of objects to keep (in pixels)\n",
    "\n",
    "holes_removed = da.map_blocks(morphology.remove_small_holes, bin_mask, max_hole_size)\n",
    "bin_mask_clean = da.map_blocks(morphology.remove_small_objects, holes_removed, min_cell_size)\n",
    "\n",
    "mask_layer_clean = viewer.add_image(bin_mask_clean, name='binary mask cleaned',colormap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise**\n",
    ">\n",
    "> Try finding good values for `max_hole_size` and `min_cell_size`, by changing the parameters above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Technical note `skimage.morphology` only works on 2D/3D image data, not on time stacks, we thus have to apply it to each frame separately, luckily `da.map_blocks()` can take care of that for us)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert semantic segmentation into instance segmentation \n",
    "\n",
    "Now we are ready to label the connected components of this image. This means that each object will be assigned a unique number. For this we can use the [`skimage.measure.labels()`](https://scikit-image.org/docs/dev/api/skimage.measure.html) function.\n",
    "To add labels to Napari, we can use the `add_label()` function.\n",
    "\n",
    "You can change the colors assigned to cells, using the shuffle button in upper left corner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#again we will need to loop over the time using map_blocks\n",
    "label_im = da.map_blocks(label, bin_mask_clean)\n",
    "label_layer = viewer.add_labels(label_im, opacity=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise**\n",
    "> \n",
    "> Inspect the segmentation. Do you find any problems? Think about what we need to do next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-process instance segmentation by separating merged cells using watershed algorithm\n",
    "\n",
    "We can see that tightly packed cells are sometimes assigned the same label.\n",
    "\n",
    "A better segmentation would assign different labels to different cells. \n",
    "\n",
    "Typically we use [watershed segmentation](https://en.wikipedia.org/wiki/Watershed_%28image_processing%29) for this purpose. This requires two steps: 1) using a so-called [distance transform](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.morphology.distance_transform_edt.html) we  calculate the distance of each pixel to the object boundary 2) we need to find center points (called *markers*) for each object. We place *markers* at the centre of each object, and these labels are expanded until they meet an edge or an adjacent marker.\n",
    "\n",
    "The trick, then, is how to find these markers. For spherical cells, there is a commonly used strategy that works quite well: we can simply find the local maxima of the distance transform (we provide the code for this below for reference).\n",
    "\n",
    "For rod shape cells, this works quite badly, though. Another way to find cell centers is by using a very high threshold for the probability map, as probabilities are typically  highest in the cell centre. Then we can use these regions as markers for the watershed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set threshold to use to find cell centers\n",
    "center_threshold = 0.95\n",
    "\n",
    "center_seg = seg_cell_sm > center_threshold\n",
    "center_markers = da.map_blocks(label, center_seg)\n",
    "    \n",
    "centre_layer = viewer.add_labels(center_markers, name='cell centre areas')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise**\n",
    "> \n",
    "> Find a good value to use for `center_threshold` such that nearby cells have different center markers (without chopping cells into parts). Spend max 5 min on this! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the center points we can further segment the image by using the watershed: we calculate the distance using [scipy.ndimage.morphology.distance_transform_edt](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.morphology.distance_transform_edt.html) and then apply the watershed using [`skimage.segmentation.watershed`](https://scikit-image.org/docs/dev/api/skimage.segmentation.html#skimage.segmentation.watershed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need a wrapper function to tranform named arguments into positional arguments to make things work with Dask\n",
    "def watershed(dist, markers, mask):\n",
    "    return segmentation.watershed(-dist, markers=markers, mask=mask)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate distance to edge of mask\n",
    "dist_transform = da.map_blocks(ndi.distance_transform_edt, bin_mask_clean)   \n",
    "\n",
    "#run watershed\n",
    "labels_final = da.map_blocks(watershed, -dist_transform, center_markers, bin_mask_clean, chunks=(1,*bin_mask_clean.shape[-2:]))\n",
    "\n",
    "#add to Napari\n",
    "watershed_layer = viewer.add_labels(labels_final, name='after watershed')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The watershed should have cut clusters of cells apart, improving the segmentation by separating nearby cells.\n",
    "\n",
    "However, you might also have seen some cells that were cut but should not have, there is often a bit of a trade-off here.\n",
    "The best solution is to improve the quality of the probability map exported by Ilastik.   \n",
    "\n",
    "There are several things you could do here to improve things:\n",
    "\n",
    "- Include fluorescent marker if possible: segmenting on fluorescent channel is almost always better than on phase contrast\n",
    "- Optimize image acquisition: the better quality the input data has, the easier and better the segmentation will be\n",
    "  - Optimize signal-to-noise ratio\n",
    "  - Optimize focus \n",
    "  - Throw out data with bad focus / exposure \n",
    "- Optimize Ilastik classification\n",
    "  - Add extra training points in problem areas\n",
    "  - Make sure your training data is diverse: include images from all your replicates\n",
    "  - Make sure you include problem frames (mediocre focus, dense cell clusters) in your training data\n",
    "  - Try segmenting on different color channels or combinations of them (e.g. phase + fluorescence)\n",
    "    - Important note: never segment on a channel you want to quantify later, i.e. do not segment on reporter gene signal, this will create a bias in  your data.\n",
    "  - Try doing deconvolution before segmentation, this will reduce the blur caused by diffraction making dense cell clusters easier to separate \n",
    "\n",
    "The fastest and easiest is often to simply go back to Ilastik and add some more training points. For time reasons we will skip this now, however."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Aside: Code to find markers for watershed algorithm for spherical cells*\n",
    "\n",
    "```python\n",
    "from skimage.feature import peak_local_max\n",
    "\n",
    "#calculate distance to edge of mask\n",
    "dist_transform = da.map_blocks(ndi.distance_transform_edt, bin_mask_clean)   \n",
    "\n",
    "#add to Napari \n",
    "dist_layer = viewer.add_image(dist_transform, name='distance transform',colormap='viridis')  \n",
    "\n",
    "#min distance between maxima, adjust this value to slightly less than 2x the radius of the smallest cells.\n",
    "min_distance = 10\n",
    "\n",
    "\n",
    "#initialize empty array of right size\n",
    "markers_list = np.empty((3,0))\n",
    "\n",
    "#find maxima in distance transform\n",
    "for t, dist_map in enumerate(dist_transform):\n",
    "    #dist_map = filters.gaussian(dist_map, 2) \n",
    "    peak_idx = peak_local_max(dist_map.compute(), min_distance=min_distance)\n",
    "    peak_idx_3d = np.insert(peak_idx.T,0,t,axis=0)\n",
    "    markers_list = np.concatenate((markers_list,peak_idx_3d),axis=1)\n",
    "    \n",
    "#add to Napari Viewer\n",
    "point_layer = viewer.add_points(\n",
    "    np.transpose(markers_list),\n",
    "    name='center points',\n",
    "    size=4,\n",
    "    n_dimensional=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing data\n",
    "\n",
    "Segmentation can be a time-consuming process so you might want to store the output on your HD. \n",
    "Here we store it as HDF5 for fast access from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can first convert to 16bit to save some space\n",
    "labels_final.astype('int16')\n",
    "\n",
    "#store as hdf5\n",
    "seg_name_hdf5 = root /  image_name.replace('.tif','_label_im.hdf5')\n",
    "labels_final.to_hdf5(seg_name_hdf5, '/labels_final')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "To load the segmentation again we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_data = h5py.File(seg_name_hdf5, 'r') #open \n",
    "labels_final_loaded = da.from_array(label_data['labels_final'], chunks=(1,-1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Cell Properties\n",
    "\n",
    "Now that we have our final segmentation, we can make measurements on them using `skimage.measure.regionprops` and `skimage.measure.regionprops_table`. These measurements include features such as area or volume, bounding boxes, and intensity statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Regionprops to extract cell properties\n",
    "`skimage.measure.regionprops` automatically measures many labeled image features. Optionally, an `intensity_image` can be supplied and intensity features are extracted per object. Note that color axis needs to be at the end!\n",
    "\n",
    "We demonstrate it's usage here by analyzing the first frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract first frame\n",
    "label_im = labels_final[0,:,:]\n",
    "image = im_stack[0,:,:]\n",
    "\n",
    "#region props need color channel to be at end\n",
    "image = np.moveaxis(image, 0, -1)\n",
    "#we add the .compute() to instruct Dask to do all the calculations at this stage\n",
    "reg_props = regionprops(label_im.compute(), intensity_image=image.compute())\n",
    "    \n",
    "print(len(reg_props))\n",
    "reg_props[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`reg_props` is a list containing the region properties of all cells present at this time. A list of all propertied can be found in the [scikit-image documentation](https://scikit-image.org/docs/dev/api/skimage.measure.html?highlight=regionprops#skimage.measure.regionprops).\n",
    "\n",
    "We can thus extract a property using `rep_props[c].[property_name]`, where `c` indicates the cell label nr.  \n",
    "\n",
    "We can for example extract the mean intensity of the cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('cell mean intensity = ', reg_props[0].intensity_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can also extract more complex info, such as the cell mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(10,5))\n",
    "axs.imshow(reg_props[0].image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Technical aside: scikit-image 0.18 adds support for [passing custom functions](https://scikit-image.org/docs/dev/api/skimage.measure.html#skimage.measure.regionprops) for region properties as `extra_properties`.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using regionprops_table to extract cell measurements\n",
    "\n",
    "`regionprops` returns a huge amount of info, but navigating it is a bit cumbersome.  \n",
    "Instead you can use [`regionprops_table`](https://scikit-image.org/docs/dev/api/skimage.measure.html?highlight=regionprops#skimage.measure.regionprops). It work in the same way, however, you will have to specify which properties you want to extract.\n",
    "\n",
    "Here we will first look at a single frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#extract first frame\n",
    "label_im = labels_final[0,:,:]\n",
    "image = im_stack[0,:,:]\n",
    "#region props need color channel to be at end\n",
    "image = np.moveaxis(image, 0, -1)\n",
    "\n",
    "#specify properties to extract \n",
    "prop_list = ['label', \n",
    "            'area', 'centroid', \n",
    "            'axis_major_length', 'axis_minor_length',\n",
    "            'mean_intensity'] \n",
    "\n",
    "#get region prop table \n",
    "#we add the .compute() to instruct Dask to do all the calculations at this stage\n",
    "rp_table = regionprops_table(label_im.compute(), intensity_image=image.compute(), properties=prop_list) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pandas Dataframe for single time point\n",
    "The output of the `regionprops_table` function can easily be converted in a [Pandas dataframe](https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html), using the [`DataFrame`](https://pandas.pydata.org/docs/user_guide/dsintro.html#dataframe) function. \n",
    "\n",
    "Pandas dataframes and R dataframes share a lot of features so if you are familiar with R you should hopefully feel right at home. For those coming from Matlab/Numpy it might take some time to get used to the Pandas syntax. Luckily there are very detailed guides available online, starting with the [Pandas Homepage](https://pandas.pydata.org).\n",
    "\n",
    "We now convert the region properties table to a Pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_table = pd.DataFrame(rp_table).set_index('label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `head()` we can have a look at the Pandas data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract cell properties for all time-points\n",
    "\n",
    "So far we only analyzed a single frame, now let's combine them all. To do this it is helpful to define a function that processes a single time point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to process single frame  \n",
    "def extract_prop_slice(t, label_im, image, prop_list):\n",
    "    #region props need color channel to be at end\n",
    "    image = np.moveaxis(image, 0, -1)\n",
    "    \n",
    "    #get region prop table\n",
    "    rp_table = regionprops_table(label_im.compute(), intensity_image=image.compute(), properties=prop_list) \n",
    "    df = pd.DataFrame(rp_table)\n",
    "    \n",
    "    #add the time index\n",
    "    df[\"frame\"] = t\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now loop over all time points calling the function we defined above and use `pandas.concat` to combine all the frames into a single data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify properties to extract \n",
    "prop_list = ['label', \n",
    "            'area', 'centroid', \n",
    "            'axis_major_length', 'axis_minor_length',\n",
    "            'mean_intensity'] \n",
    "\n",
    "#loop over all frames\n",
    "df_list = [extract_prop_slice(t, label, image, prop_list) for t, (label, image) in enumerate(zip(labels_final, im_stack))]\n",
    "\n",
    "#combine into single dataframe\n",
    "info_table_all = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Technical aside: Python has some nice tools which allows you to write a full for loop in a single line:*\n",
    "\n",
    "*`df_list = [extract_prop_slice(t, label, image, prop_list) for t, (label, image) in enumerate(zip(labels_final, im_stack))]`*\n",
    "\n",
    "*In this code `for t, (label, image) in enumerate(zip(labels_final, im_stack))` will iterate simultaneously through the frames contained in `labels_final` and `im_stack` (`zip` takes care of this) and in addition it will provide the index of the iteration (`enumerate` takes care of this). These values are then passed on to the `extract_prop_slice()` function. By placing everything between square brackets in is returned as a list* \n",
    "\n",
    "*[Here](https://www.w3schools.com/python/python_lists_comprehension.asp) you can find more details of this so called list comprehension.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show start of dataframe\n",
    "info_table_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show end of dataframe\n",
    "info_table_all.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing data\n",
    "We can store the dataframe to HDD in e.g. `.pkl` or `.csv` format. [See here](https://pandas.pydata.org/docs/user_guide/io.html) for all supported formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name_pkl = root /  image_name.replace('.tif','_cellprop.pkl')\n",
    "info_table_all.to_pickle(data_name_pkl)\n",
    "\n",
    "data_name_csv = root /  image_name.replace('.tif','_cellprop.csv')\n",
    "info_table_all.to_csv(data_name_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "And we can load it again. [See here](https://pandas.pydata.org/docs/user_guide/io.html) for all supported formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_table_loaded = pd.read_pickle(data_name_pkl)\n",
    "info_table_loaded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Pandas Dataframes\n",
    "Now let's analyze some data. You can manipulate Pandas data frames and e.g. extract a column.  \n",
    "An extracted column is know as a [Series](https://pandas.pydata.org/docs/user_guide/dsintro.html#series)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_length = info_table_all['axis_major_length'] #extract a column \n",
    "print('data type of extracted column = ', type(cell_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also add new columns, either from a Series, Vector data, or constant value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_length = info_table_all['axis_major_length'] #extract a column \n",
    "cell_width = info_table_all['axis_minor_length'] #extract a column \n",
    "\n",
    "#add new column based on ratio of two Series\n",
    "info_table_all[\"aspect_ratio\"] = cell_length/cell_width\n",
    "\n",
    "#show output\n",
    "info_table_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a quick summary of the data using the [`describe` function](https://pandas.pydata.org/docs/getting_started/intro_tutorials/06_calculate_statistics.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_table_all.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also filter on rows and for example extract all cells that fulfill a certain requirement, for example we can sort out the largest 0.1% of cells using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find 99.9% percentile of cell lengths:\n",
    "size_tr = info_table_all['axis_major_length'].quantile(0.999)\n",
    "\n",
    "#select biggest cells:  \n",
    "huge_cells = info_table_all[info_table_all['axis_major_length']>size_tr]\n",
    "huge_cells.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details of how to use Pandas, see examples below or consult the extensive [documentation online](https://pandas.pydata.org/docs/user_guide/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting and analyzing cell properties using Pandas, Matplotlib, and Seaborn \n",
    "\n",
    "### Cell number over time\n",
    "We will first look at how the number of cells changes over time.\n",
    "\n",
    "We use the [`groupby`](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#splitting-an-object-into-groups) function on the `frame` property to group cells based on their frame. We can then count the number of cells at each frame by calling the `size()` property.\n",
    "\n",
    "The result is a [Pandas Data Series](https://pandas.pydata.org/docs/user_guide/dsintro.html#series). We can directly plot the result using the build-in plot function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_num_t = info_table_all.groupby('frame').size() #output is pandas data series\n",
    "cell_num_t.plot(xlabel='frame',ylabel='# of cells',figsize=(10,5)) #use build in plot function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average cell properties over time\n",
    "Next we will look at how the average properties of a cell, such as their fluorescent intensity, changes over time.\n",
    "\n",
    "Again we use the [`groupby`](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#splitting-an-object-into-groups) function on the `frame` property to group cells based on their frame. We can then calculate the average value for each group by calling the `mean()` function.\n",
    "The output in this case is a Pandas dataframe, with shows the average value over all cells contained in a given frame (each row is a frame). \n",
    "\n",
    "[Here](https://pbpython.com/groupby-agg.html) you can find an overview of how to group and aggregate data in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "av_prop = info_table_all.groupby('frame').mean()\n",
    "av_prop['frame'] = av_prop.index\n",
    "av_prop.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the result in two ways: using [Matplotlib](https://matplotlib.org/stable/index.html) or [seaborn](https://seaborn.pydata.org/index.html).\n",
    "\n",
    "Matplotlib is a lower level package, giving you a lot of freedom but requiring quite a bit of code to make things look nice.\n",
    "\n",
    "Seaborn is a higher level package, making it easier to make nice looking figures, at the cost of some flexibility.\n",
    "\n",
    "First we show you Matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot with Matplotlib\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax.plot(av_prop['frame'],av_prop['mean_intensity-0'],label='channel 0')\n",
    "ax.plot(av_prop['frame'],av_prop['mean_intensity-1'],label='channel 1')\n",
    "ax.set_xlabel('frame nr')\n",
    "ax.set_ylabel('fluorescent intensity')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting with Seaborn\n",
    "\n",
    "Now let's look at Seaborn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot with Seaborn\n",
    "p = sns.lineplot(data=av_prop[[\"mean_intensity-0\", \"mean_intensity-1\"]])\n",
    "p.set_ylabel(\"fluorescent intensity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly we can look at distributions and scatter plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(12,6))\n",
    "sns.histplot(ax=axs[0], data=info_table_all[[\"mean_intensity-0\", \"mean_intensity-1\"]])\n",
    "sns.scatterplot(ax=axs[1], data=info_table_all, x=\"mean_intensity-0\", y=\"mean_intensity-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seaborn has also many mare advanced functionalities. For example you can automatically make a facet plot to make a separate plot for each group. Here we visualize how the distribution of RFP intensities changes over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(info_table_all, col=\"frame\", col_wrap=10, height=2)\n",
    "g.map(sns.histplot, \"mean_intensity-0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details, you can consult the [Example gallery](https://seaborn.pydata.org/examples/index.html) or [Tutorial section](https://seaborn.pydata.org/tutorial.html) on the Seaborn website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time for some biology\n",
    "The data we have been looking at is from a synthetic microbial cross-feeding community consisting of two auxotrophic strains of E. coli that can only grow by exchanging Amino-Acids with each other. We are interested in understanding the dynamics of this community, and as a first question we like to know how the frequency of the two cell types changes over time. It is now your task to try to answer this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Exercise\n",
    "> We are interested in the dynamics of the community, and would like to know how the fraction of red cells changes over time.   \n",
    "> Try to come up with a way to calculate this.\n",
    "> \n",
    "> Hints:\n",
    "> - Think about how you can tell red and green cells apart in a reliable way\n",
    "> - Classify cells as either red or green\n",
    "> - Calculate the fraction of red cells over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enter code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1\n",
    "To see the solution, uncomment (remove `#`) the `load` line below and run the cell twice (first time will load the code second time will run it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../Solutions/p0_classify_sol1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2\n",
    "To see the solution, uncomment (remove `#`) the `load` line below and run the cell twice (first time will load the code second time will run it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../Solutions/p0_classify_sol2.py"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
